{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration and competition\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "In this notebook, we solve a prebuilt `Tennis`environment from the [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) engine. Our agents learn to play using a vanilla implementation of the `Multi-Agent Deep Deterministic Policy Gradients` (MADDPG) algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Description of the environment and example run\n",
    "\n",
    "The simulation contains two agents, two tennis rackets that can move forward and backwards, as well as jump. The environment also contains a ball that moves in the court, and which the agents will try to bounce over the net. The environment is better described as follows:\n",
    "\n",
    "- **State space**: The state space dimension `8` for each agent. These 8 dimensions represent position and velocity (in `x`, `y`) coordinates for both the agent and the ball. The state representation available in the environment contains 3 stacked state vectors, in order to provide info on the actual progression of the ball. Each agent only have access to their own (local) state observations.\n",
    "\n",
    "\n",
    "- **Action space**: Two continuous actions are available, corresponding to movement toward (or away from) the net, and jumping.\n",
    "\n",
    "\n",
    "- **Rewards**: If an agent hits the ball over the net, it receives a reward of `+0.1`. If an agent lets a ball hit the ground or hits the ball out of bounds, it receives a reward of `-0.01`. Thus, the goal of each agent is to keep the ball in play.\n",
    "\n",
    "\n",
    "- **Episodes**: The task is episodic and the reward is measured as the maximum total reward recieved by any of the agents. The environment is considered solved when a mean score of `+0.5` is achieved over `100` consecutive episodes.\n",
    "\n",
    "---\n",
    "\n",
    "### 2.1. Exploring the environment\n",
    "\n",
    "We explore the environment in the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n",
      "Total dimensions in state: 24\n",
      "States look like: [ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         -6.65278625 -1.5\n",
      " -0.          0.          6.83172083  6.         -0.          0.        ]\n",
      "Action space dimension: 2\n"
     ]
    }
   ],
   "source": [
    "##########################\n",
    "#### THE ENVIRONMENT #####\n",
    "##########################\n",
    "\n",
    "# Initialize the environment and get the default brain\n",
    "env = UnityEnvironment(file_name=\"Tennis_Windows_x86_64//Tennis.x86_64\")\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "\n",
    "##########################\n",
    "#### THE STATE SPACE #####\n",
    "##########################\n",
    "\n",
    "state = env_info.vector_observations[0]\n",
    "print('Total dimensions in state:', len(state))\n",
    "print('States look like:', state)\n",
    "\n",
    "\n",
    "######################\n",
    "## THE ACTION SPACE ##\n",
    "######################\n",
    "\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Action space dimension:', action_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Example run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how a random agent would perform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-1f10e5c5be99>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;31m# Reset the environment, get current state for each agent and initialize scores\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0menv_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbrain_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mstates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv_info\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvector_observations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_agents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "# Play 5 episodes\n",
    "for i in range(5):\n",
    "    \n",
    "    # Reset the environment, get current state for each agent and initialize scores\n",
    "    env_info = env.reset(train_mode=False)[brain_name]\n",
    "    states = env_info.vector_observations\n",
    "    scores = np.zeros(num_agents)\n",
    "    dones = [False, False]\n",
    "    \n",
    "    # Play the episode\n",
    "    while not np.any(dones):\n",
    "        \n",
    "        # Select an action (for each agent) and clip them between -1 and 1\n",
    "        actions = np.random.randn(num_agents, action_size) \n",
    "        actions = np.clip(actions, -1, 1)\n",
    "        \n",
    "        # Execute the actions\n",
    "        env_info = env.step(actions)[brain_name]   \n",
    "        \n",
    "        # Get next states, rewards, and info on whether the episode is over\n",
    "        next_states = env_info.vector_observations\n",
    "        rewards = env_info.rewards\n",
    "        dones = env_info.local_done\n",
    "        \n",
    "        # Update total scores and assign next states as the new states\n",
    "        scores += env_info.rewards\n",
    "        states = next_states\n",
    "        \n",
    "    print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training an agent (restart the kernel before running)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "from DDPG import *\n",
    "from collections import deque\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize the environment and assign the brain\n",
    "env = UnityEnvironment(file_name=\"Tennis_Windows_x86_64//Tennis.x86_64\")\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "# Reset environment and assign initial state\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "state = env_info.vector_observations[0]\n",
    "num_agents = len(env_info.agents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the two agents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "agents = [Agent(agent_index = 1,\n",
    "                state_size = len(state),\n",
    "                action_size = brain.vector_action_space_size, \n",
    "                seed = 42),\n",
    "          Agent(agent_index = 2,\n",
    "                state_size = len(state),\n",
    "                action_size = brain.vector_action_space_size, \n",
    "                seed = 42)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 20\tAverage Score: 0.01\n",
      "[array([-0.94708574,  1.        ], dtype=float32), array([1., 1.], dtype=float32)]\n",
      "Episode 40\tAverage Score: 0.01\n",
      "[array([-1.        ,  0.19265693], dtype=float32), array([-0.32302213,  1.        ], dtype=float32)]\n",
      "Episode 60\tAverage Score: 0.01\n",
      "[array([-0.80480146, -0.61080635], dtype=float32), array([-0.92642397,  1.        ], dtype=float32)]\n",
      "Episode 80\tAverage Score: 0.01\n",
      "[array([-1.,  1.], dtype=float32), array([-0.8206182,  1.       ], dtype=float32)]\n",
      "Episode 100\tAverage Score: 0.01\n",
      "[array([-0.89525884, -0.3548652 ], dtype=float32), array([-1.,  1.], dtype=float32)]\n",
      "Episode 120\tAverage Score: 0.00\n",
      "[array([ 1., -1.], dtype=float32), array([-0.5297426,  1.       ], dtype=float32)]\n",
      "Episode 140\tAverage Score: 0.00\n",
      "[array([-0.6331079 , -0.32965988], dtype=float32), array([-1.      ,  0.958004], dtype=float32)]\n",
      "Episode 160\tAverage Score: 0.00\n",
      "[array([ 1., -1.], dtype=float32), array([-1.       ,  0.9045898], dtype=float32)]\n",
      "Episode 180\tAverage Score: 0.00\n",
      "[array([ 1., -1.], dtype=float32), array([-1.,  1.], dtype=float32)]\n",
      "Episode 200\tAverage Score: 0.01\n",
      "[array([-1.        ,  0.72870934], dtype=float32), array([-0.7617527,  1.       ], dtype=float32)]\n",
      "Episode 220\tAverage Score: 0.01\n",
      "[array([-0.09768665, -1.        ], dtype=float32), array([-0.5968849 ,  0.37388885], dtype=float32)]\n",
      "Episode 240\tAverage Score: 0.01\n",
      "[array([-1.       , -0.9765399], dtype=float32), array([-1.,  1.], dtype=float32)]\n",
      "Episode 260\tAverage Score: 0.01\n",
      "[array([ 1., -1.], dtype=float32), array([-1.        ,  0.13489062], dtype=float32)]\n",
      "Episode 280\tAverage Score: 0.01\n",
      "[array([ 1., -1.], dtype=float32), array([-0.79911166,  1.        ], dtype=float32)]\n",
      "Episode 300\tAverage Score: 0.01\n",
      "[array([ 1.        , -0.96991336], dtype=float32), array([-0.8063761,  1.       ], dtype=float32)]\n",
      "Episode 320\tAverage Score: 0.01\n",
      "[array([-1.      , -0.800554], dtype=float32), array([-0.70533615,  1.        ], dtype=float32)]\n",
      "Episode 340\tAverage Score: 0.00\n",
      "[array([-1., -1.], dtype=float32), array([-1.,  1.], dtype=float32)]\n",
      "Episode 360\tAverage Score: 0.00\n",
      "[array([-1., -1.], dtype=float32), array([-1.,  1.], dtype=float32)]\n",
      "Episode 380\tAverage Score: 0.00\n",
      "[array([-0.95779556, -1.        ], dtype=float32), array([-1.,  1.], dtype=float32)]\n",
      "Episode 400\tAverage Score: 0.00\n",
      "[array([-0.94318026, -1.        ], dtype=float32), array([-1.       ,  0.8695008], dtype=float32)]\n",
      "Episode 420\tAverage Score: 0.00\n",
      "[array([-0.61902225, -1.        ], dtype=float32), array([-0.9767462 ,  0.91004974], dtype=float32)]\n",
      "Episode 440\tAverage Score: 0.00\n",
      "[array([-1., -1.], dtype=float32), array([-1.        ,  0.99509597], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "n_episodes=5000\n",
    "eps_start=0.6\n",
    "eps_end=0.0005 \n",
    "eps_decay=0.995\n",
    "\n",
    "\n",
    "''' Train a Deep Deterministic Policy Gradient agent to solve the environment '''\n",
    "\n",
    "scores = []                        # list containing scores from each episode\n",
    "scores_window = deque(maxlen=100)  # last 100 scores\n",
    "eps = eps_start                    # initialize epsilon\n",
    "\n",
    "# For each one of the training episodes\n",
    "for i_episode in range(1, n_episodes+1):\n",
    "\n",
    "    # Restart the environment and the score\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    running_scores = [0, 0]\n",
    "\n",
    "    # Get agent's current states (two-element lists with local observations)\n",
    "    states = env_info.vector_observations\n",
    "    dones = [False, False]\n",
    "\n",
    "    # Play the episode\n",
    "    while not np.any(dones):\n",
    "\n",
    "        # Get the noise-greedy actions for each agent\n",
    "        actions = [agent.act(states[i], eps).data.numpy() for i, agent in enumerate(agents)]\n",
    "\n",
    "        # Take the action and store environment output\n",
    "        env_info = env.step(actions)[brain_name] \n",
    "        next_states = env_info.vector_observations\n",
    "        rewards = env_info.rewards\n",
    "        dones = env_info.local_done\n",
    "\n",
    "        # Update agent's knowledge\n",
    "        for agent_index, agent in enumerate(agents): \n",
    "            agent.step(agent_index, states, actions, rewards, next_states, dones)\n",
    "            running_scores[agent_index] += rewards[agent_index]\n",
    "\n",
    "        # Move onto next state\n",
    "        states = next_states\n",
    "\n",
    "    # When the episode is done, save scores into our lists, and decrease the epsilon for the next episode\n",
    "    scores_window.append(np.max(running_scores))\n",
    "    scores.append(np.max(running_scores))              \n",
    "    eps = max(eps_end, eps_decay*eps)\n",
    "\n",
    "    if i_episode % 20 == 0:\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "        print(actions)\n",
    "\n",
    "\n",
    "    # When the agent is trained, we save the weights of the Q-network and exit the routine\n",
    "    if np.mean(scores_window)>=0.5:\n",
    "        print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "        break\n",
    "\n",
    "for i, agent in enumerate(agents): torch.save(agent.actor_local.state_dict(), 'trained_agent_{}.pth'.format(i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Persist agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, agent in enumerate(agents): torch.save(agent.actor_local.state_dict(), 'trained_agent_{}.pth'.format(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-7c1d5f1060b3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Score'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Episode #'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Evolution of agent's average performance\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.title(\"Evolution of agent's average performance\")\n",
    "plt.grid(alpha=0.8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Our agent in action! (restart the kernel before running)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We leverage the network weights learned in section 3 to create an intelligent agent, and watch it interact with its environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "from DDPG import *\n",
    "from collections import deque\n",
    "import torch\n",
    "\n",
    "# Initialize the environment and assign the brain\n",
    "env = UnityEnvironment(file_name=\"Tennis_Windows_x86_64//Tennis.x86_64\")\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "env_info = env.reset(train_mode=False)[brain_name] \n",
    "states = env_info.vector_observations  \n",
    "num_agents = len(env_info.agents)\n",
    "\n",
    "# Start an agent and assign it the trained network\n",
    "agents = [Agent(state_size = len(states[0]),\n",
    "                action_size = brain.vector_action_space_size, \n",
    "                seed = 42),\n",
    "          Agent(state_size = len(states[0]),\n",
    "                action_size = brain.vector_action_space_size, \n",
    "                seed = 42)]\n",
    "\n",
    "for i, agent in enumerate(agents): agent.actor_local.load_state_dict(torch.load('trained_agent_{}.pth'.format(i)))\n",
    "\n",
    "    \n",
    "for i_episode in range(10):\n",
    "    \n",
    "    # Reset the environment, assign the initial state and initialize the score\n",
    "    env_info = env.reset(train_mode=False)[brain_name] \n",
    "    states = env_info.vector_observations  \n",
    "    num_agents = len(env_info.agents)\n",
    "    dones = [False, False]\n",
    "    scores = np.zeros(num_agents)\n",
    "    while not np.any(dones):\n",
    "\n",
    "        # Get the noise-greedy actions for each agent\n",
    "        actions = [agent.act(states[i], 0).data.numpy() for i, agent in enumerate(agents)]\n",
    "\n",
    "        # Take the action and store environment output\n",
    "        env_info = env.step(actions)[brain_name] \n",
    "        next_states = env_info.vector_observations\n",
    "        rewards = env_info.rewards\n",
    "        dones = env_info.local_done\n",
    "\n",
    "        # Update agent's knowledge\n",
    "        for i, agent in enumerate(agents): scores[i] += rewards[i]\n",
    "\n",
    "        # Move onto next state\n",
    "        state = next_states\n",
    "\n",
    "#print(\"Final score: {}\".format(score))\n",
    "env.close() \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
